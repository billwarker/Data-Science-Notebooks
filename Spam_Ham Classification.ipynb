{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam / Ham Classification\n",
    "\n",
    "This notebook is my attempt at creating a spam classifier using basic machine learning models. I'm going to train my classification algorithms on a small dataset of 1000 ham (real) and 1000 spam emails downloaded from Apache Spam Assassin. Each email is an individual text file, and the entire dataset consists of approximately 6000 files spread across 8 folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = r'C:\\Users\\Will\\Desktop\\Datasets\\Apache Spam Ham\\Spam Ham Dataset'\n",
    "folders = ['easy_ham', 'spam3'] # Drawing from these two folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_email(email):\n",
    "    with open(email, 'r') as file:\n",
    "        content = file.readlines()\n",
    "        file.close()\n",
    "    string = ''\n",
    "    for x in content:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_path = os.path.join(root, folders[1])\n",
    "sample_email = os.path.join(sample_path, os.listdir(sample_path)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From ilug-admin@linux.ie  Tue Aug  6 11:51:02 2002\n",
      "\n",
      "Return-Path: <ilug-admin@linux.ie>\n",
      "\n",
      "Delivered-To: yyyy@localhost.netnoteinc.com\n",
      "\n",
      "Received: from localhost (localhost [127.0.0.1])\n",
      "\n",
      "\tby phobos.labs.netnoteinc.com (Postfix) with ESMTP id 9E1F5441DD\n",
      "\n",
      "\tfor <jm@localhost>; Tue,  6 Aug 2002 06:48:09 -0400 (EDT)\n",
      "\n",
      "Received: from phobos [127.0.0.1]\n",
      "\n",
      "\tby localhost with IMAP (fetchmail-5.9.0)\n",
      "\n",
      "\tfor jm@localhost (single-drop); Tue, 06 Aug 2002 11:48:09 +0100 (IST)\n",
      "\n",
      "Received: from lugh.tuatha.org (root@lugh.tuatha.org [194.125.145.45]) by\n",
      "\n",
      "    dogma.slashnull.org (8.11.6/8.11.6) with ESMTP id g72LqWv13294 for\n",
      "\n",
      "    <jm-ilug@jmason.org>; Fri, 2 Aug 2002 22:52:32 +0100\n",
      "\n",
      "Received: from lugh (root@localhost [127.0.0.1]) by lugh.tuatha.org\n",
      "\n",
      "    (8.9.3/8.9.3) with ESMTP id WAA31224; Fri, 2 Aug 2002 22:50:17 +0100\n",
      "\n",
      "Received: from bettyjagessar.com (w142.z064000057.nyc-ny.dsl.cnc.net\n",
      "\n",
      "    [64.0.57.142]) by lugh.tuatha.org (8.9.3/8.9.3) with ESMTP id WAA31201 for\n",
      "\n",
      "    <ilug@linux.ie>; Fri, 2 Aug 2002 22:50:11 +0100\n",
      "\n",
      "X-Authentication-Warning: lugh.tuatha.org: Host w142.z064000057.nyc-ny.dsl.cnc.net\n",
      "\n",
      "    [64.0.57.142] claimed to be bettyjagessar.com\n",
      "\n",
      "Received: from 64.0.57.142 [202.63.165.34] by bettyjagessar.com\n",
      "\n",
      "    (SMTPD32-7.06 EVAL) id A42A7FC01F2; Fri, 02 Aug 2002 02:18:18 -0400\n",
      "\n",
      "Message-Id: <1028311679.886@0.57.142>\n",
      "\n",
      "Date: Fri, 02 Aug 2002 23:37:59 0530\n",
      "\n",
      "To: ilug@linux.ie\n",
      "\n",
      "From: \"Start Now\" <startnow2002@hotmail.com>\n",
      "\n",
      "MIME-Version: 1.0\n",
      "\n",
      "Content-Type: text/plain; charset=\"US-ASCII\"; format=flowed\n",
      "\n",
      "Subject: [ILUG] STOP THE MLM INSANITY\n",
      "\n",
      "Sender: ilug-admin@linux.ie\n",
      "\n",
      "Errors-To: ilug-admin@linux.ie\n",
      "\n",
      "X-Mailman-Version: 1.1\n",
      "\n",
      "Precedence: bulk\n",
      "\n",
      "List-Id: Irish Linux Users' Group <ilug.linux.ie>\n",
      "\n",
      "X-Beenthere: ilug@linux.ie\n",
      "\n",
      "\n",
      "\n",
      "Greetings!\n",
      "\n",
      "\n",
      "\n",
      "You are receiving this letter because you have expressed an interest in \n",
      "\n",
      "receiving information about online business opportunities. If this is \n",
      "\n",
      "erroneous then please accept my most sincere apology. This is a one-time \n",
      "\n",
      "mailing, so no removal is necessary.\n",
      "\n",
      "\n",
      "\n",
      "If you've been burned, betrayed, and back-stabbed by multi-level marketing, \n",
      "\n",
      "MLM, then please read this letter. It could be the most important one that \n",
      "\n",
      "has ever landed in your Inbox.\n",
      "\n",
      "\n",
      "\n",
      "MULTI-LEVEL MARKETING IS A HUGE MISTAKE FOR MOST PEOPLE\n",
      "\n",
      "\n",
      "\n",
      "MLM has failed to deliver on its promises for the past 50 years. The pursuit \n",
      "\n",
      "of the \"MLM Dream\" has cost hundreds of thousands of people their friends, \n",
      "\n",
      "their fortunes and their sacred honor. The fact is that MLM is fatally \n",
      "\n",
      "flawed, meaning that it CANNOT work for most people.\n",
      "\n",
      "\n",
      "\n",
      "The companies and the few who earn the big money in MLM are NOT going to \n",
      "\n",
      "tell you the real story. FINALLY, there is someone who has the courage to \n",
      "\n",
      "cut through the hype and lies and tell the TRUTH about MLM.\n",
      "\n",
      "\n",
      "\n",
      "HERE'S GOOD NEWS\n",
      "\n",
      "\n",
      "\n",
      "There IS an alternative to MLM that WORKS, and works BIG! If you haven't yet \n",
      "\n",
      "abandoned your dreams, then you need to see this. Earning the kind of income \n",
      "\n",
      "you've dreamed about is easier than you think!\n",
      "\n",
      "\n",
      "\n",
      "With your permission, I'd like to send you a brief letter that will tell you \n",
      "\n",
      "WHY MLM doesn't work for most people and will then introduce you to \n",
      "\n",
      "something so new and refreshing that you'll wonder why you haven't heard of \n",
      "\n",
      "this before.\n",
      "\n",
      "\n",
      "\n",
      "I promise that there will be NO unwanted follow up, NO sales pitch, no one \n",
      "\n",
      "will call you, and your email address will only be used to send you the \n",
      "\n",
      "information. Period.\n",
      "\n",
      "\n",
      "\n",
      "To receive this free, life-changing information, simply click Reply, type \n",
      "\n",
      "\"Send Info\" in the Subject box and hit Send. I'll get the information to you \n",
      "\n",
      "within 24 hours. Just look for the words MLM WALL OF SHAME in your Inbox.\n",
      "\n",
      "\n",
      "\n",
      "Cordially,\n",
      "\n",
      "\n",
      "\n",
      "Siddhi\n",
      "\n",
      "\n",
      "\n",
      "P.S. Someone recently sent the letter to me and it has been the most \n",
      "\n",
      "eye-opening, financially beneficial information I have ever received. I \n",
      "\n",
      "honestly believe that you will feel the same way once you've read it. And \n",
      "\n",
      "it's FREE!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "This email is NEVER sent unsolicited.  THIS IS NOT \"SPAM\". You are receiving \n",
      "\n",
      "this email because you EXPLICITLY signed yourself up to our list with our \n",
      "\n",
      "online signup form or through use of our FFA Links Page and E-MailDOM \n",
      "\n",
      "systems, which have EXPLICIT terms of use which state that through its use \n",
      "\n",
      "you agree to receive our emailings.  You may also be a member of a Altra \n",
      "\n",
      "Computer Systems list or one of many numerous FREE Marketing Services and as \n",
      "\n",
      "such you agreed when you signed up for such list that you would also be \n",
      "\n",
      "receiving this emailing.\n",
      "\n",
      "Due to the above, this email message cannot be considered unsolicitated, or \n",
      "\n",
      "spam.\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-- \n",
      "\n",
      "Irish Linux Users' Group: ilug@linux.ie\n",
      "\n",
      "http://www.linux.ie/mailman/listinfo/ilug for (un)subscription information.\n",
      "\n",
      "List maintainer: listmaster@linux.ie\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_email(sample_email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Testing sets\n",
    "\n",
    "First, I'm going to label the 2000 files I'll use for my dataset and move them into separate directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rename files to be prefixed with 'SPAM' or 'HAM' before merging into respective training and test dirs\n",
    "\n",
    "os.chdir(root)\n",
    "num_files = 1000 # 1000 spam, 1000 ham\n",
    "\n",
    "for folder in folders:\n",
    "    path = (os.path.join(root, folder))\n",
    "    os.chdir(path)\n",
    "    os.makedirs('Labeled') # Create new folder 'Labeled' to hold new labeled files\n",
    "    \n",
    "    # take 500 files from each dir\n",
    "    for file in os.listdir()[0:num_files]:\n",
    "        if file == 'Labeled':\n",
    "            pass\n",
    "        elif folder.startswith('easy'):\n",
    "            os.rename(os.path.join(path, file), os.path.join(os.path.join(path, 'Labeled'), 'HAM_' + file))\n",
    "        else:\n",
    "            os.rename(os.path.join(path, file), os.path.join(os.path.join(path, 'Labeled'), 'SPAM_' + file))\n",
    "            \n",
    "        # Ham emails start with HAM_ and Spam emails start with SPAM_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'll create training and testing folders to move the emails into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.mkdir(os.path.join(root, 'Train')) # Create Train/Test directories\n",
    "os.mkdir(os.path.join(root, 'Test'))\n",
    "\n",
    "file_paths = [] # list of complete file paths for emails in their respective 'Labeled' directories\n",
    "file_names = [] # list of only the file names\n",
    "for folder in folders:\n",
    "    path = (os.path.join(root, folder))\n",
    "    os.chdir(os.path.join(path, 'Labeled'))\n",
    "    for file in os.listdir():\n",
    "        file_paths.append(os.path.abspath(file)) # populate both lists\n",
    "        file_names.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2000 files in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_count = len(file_names)\n",
    "file_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll define the size of the training set to be 80% of the entire dataset, so 1600 emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 1600\n",
      "Size of testing set: 400\n"
     ]
    }
   ],
   "source": [
    "train_size = int(file_count*0.8) # 80%\n",
    "print('Size of training set: ' + str(train_size))\n",
    "print('Size of testing set: ' + str(file_count - train_size))\n",
    "\n",
    "train_path = os.path.join(root, 'Train') # Create paths to the training and testing directories\n",
    "test_path = os.path.join(root, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'll randomly split the dataset into the training and testing folders based on the defined train size. Since I'm just going to use numpy to generate random numbers it won't be a precise 50/50 split, but the slight difference between the two will have a negligible impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnd.seed(42) # seed the generator to duplicate results\n",
    "file_index = np.arange(len(file_names)) # 0, 1, 2, 3, 4, 5, ..., 2000\n",
    "rnd.shuffle(file_index) # randomly shuffle all the files in the entire dataset\n",
    "\n",
    "for file in file_index[:train_size]: # emails 1-1000\n",
    "    try:\n",
    "        os.rename(file_paths[file], os.path.join(train_path, file_names[file]))\n",
    "    except:\n",
    "        print('Copy error') # **\n",
    "        \n",
    "for file in file_index[train_size:]: # emails 1001-2000\n",
    "    try:\n",
    "        os.rename(file_paths[file], os.path.join(test_path, file_names[file]))\n",
    "    except:\n",
    "        print('Test copy error') # **\n",
    "        \n",
    "# ** Previous implementations of this method resulting in some files not being copied for some unknown reason. When I changed the\n",
    "#    number of files that I was going to draw from each original spam/ham folder, this problem stopped occurring. I left it in\n",
    "#    there just in case. A bug that seemed to iron itself out? For the purpose of this notebook, I decided to not investigate the\n",
    "#    problem any further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Will\\Desktop\\Datasets\\Apache Spam Ham\\Spam Ham Dataset\\Train\n",
      "Ham: 818, Spam: 782\n",
      "C:\\Users\\Will\\Desktop\\Datasets\\Apache Spam Ham\\Spam Ham Dataset\\Test\n",
      "Ham: 182, Spam: 218\n"
     ]
    }
   ],
   "source": [
    "for path in [train_path, test_path]:\n",
    "    print(path)\n",
    "    spam = 0\n",
    "    ham = 0\n",
    "    for x in os.listdir(path):\n",
    "        if x.startswith('HAM'):\n",
    "            ham += 1\n",
    "        else:\n",
    "            spam += 1\n",
    "    print(('Ham: {}, Spam: {}').format(ham, spam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features and labels from the Training set\n",
    "\n",
    "The algorithms I'm using to generate the features from the training set's emails are mostly not my own; they're ones I found in a similar Spam classification exercise from KDnuggets.com. (http://www.kdnuggets.com/2017/03/email-spam-filtering-an-implementation-with-python-and-scikit-learn.html) I've tweaked them to fit the context I'm working in.\n",
    "\n",
    "The function below parses every file in the training dictionary and extracts each word. It then creates a dictionary of all the words found and counts the number of times they come up in the entire data set. I'm going to use the 3000 most common words as features for my algorithm, but there are some caveats. I'm going to ignore words that contain numbers or symbols of any type of formating in them, which show up in a lot of the emails (e.g. ASCII art and HTML formating). I'm also going to ignore all stop words (i.e. common words like 'it', 'the', 'is') by running the dictionary results through the stopwords dataset found in the NLTK (Natural Language Toolkit) module. Finally, I ignore all words that are a single character long (e.g. random letters and spaces). It returns a list of the 3000 most common words and the number of times they occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter # counts frequency of words\n",
    "from nltk.corpus import stopwords # stopwords dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_dictionary(train_dir):\n",
    "    emails = [os.path.join(train_dir, f) for f in os.listdir(train_dir)]\n",
    "    all_words = []\n",
    "    for mail in emails:\n",
    "        with open(mail) as m:\n",
    "            for i, line in enumerate(m):\n",
    "                words = line.split()\n",
    "                all_words += words\n",
    "        m.close()\n",
    "                \n",
    "    dictionary = Counter(all_words)            \n",
    "    list_to_remove = list(dictionary)\n",
    "    stop_words = set(stopwords.words('english')) # grab the English list of stop words from the NLTK module\n",
    "    \n",
    "    for item in list_to_remove:\n",
    "        if item in stop_words: # remove common stopwords - both ham and spam will have lots of these\n",
    "            del dictionary[item]\n",
    "        if item.isalpha() == False: # remove words that aren't entirely alphabetical characters\n",
    "            del dictionary[item]\n",
    "        elif len(item) == 1: # remove words with a single character (spaces, symbols)\n",
    "            del dictionary[item]\n",
    "    dictionary = dictionary.most_common(3000)\n",
    "    \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's pass the function the training directory and see which words came up most often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 6504),\n",
       " ('ESMTP', 4757),\n",
       " ('Sep', 3765),\n",
       " ('Aug', 2582),\n",
       " ('localhost', 2127),\n",
       " ('Jul', 2062),\n",
       " ('The', 1865),\n",
       " ('From', 1543),\n",
       " ('Oct', 1472),\n",
       " ('May', 1355)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dictionary = make_dictionary(train_path)\n",
    "word_dictionary[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of the most common words are those that show up in email formatting: 'id', 'ESMTP', 'SMTP', 'email'. Some other common words are the names of months: 'Jun', 'Jul', 'Aug', 'Sep' - these occurences are likely the months when the individual emails were sent. Besides those, there are a few words that seem to be indicative of spam: 'bulk', 'receive', 'money', 'send', etc.\n",
    "\n",
    "Now, we need to translate this list into a feature matrix. The function below creates an $M  x  N$ matrix where $M$ is the number of emails being passed to it (training or testing folder) and $N$ is the number of entries in our word dictionary (3000 features). It then parses each email for the words in the dictionary, adding them to the email rows' respective feature columns every time they're found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(mail_dir, dictionary):\n",
    "    count = 0\n",
    "    files = [os.path.join(mail_dir, file) for file in os.listdir(mail_dir)]\n",
    "    features_matrix = np.zeros((len(files), len(dictionary)))\n",
    "    docID = 0 # matrix row\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            for index, line in enumerate(f):\n",
    "                words = line.split()\n",
    "                for word in words:\n",
    "                    wordID = 0 # matrix column\n",
    "                    for i, d in enumerate(dictionary):\n",
    "                        if d[0] == word:\n",
    "                            wordID = i\n",
    "                            features_matrix[docID, wordID] += words.count(word) # add total occurences of word to the feature\n",
    "        docID += 1 # move to next row\n",
    "        f.close()\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(('{} files processed').format(str(count)))\n",
    "    return features_matrix\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 files processed\n",
      "200 files processed\n",
      "300 files processed\n",
      "400 files processed\n",
      "500 files processed\n",
      "600 files processed\n",
      "700 files processed\n",
      "800 files processed\n",
      "900 files processed\n",
      "1000 files processed\n",
      "1100 files processed\n",
      "1200 files processed\n",
      "1300 files processed\n",
      "1400 files processed\n",
      "1500 files processed\n",
      "1600 files processed\n"
     ]
    }
   ],
   "source": [
    "X = extract_features(train_path, word_dictionary) # matrix of training features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This short function parses a folder and returns a target array. 1 if an email starts with 'HAM', else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_targets(mail_dir):\n",
    "    targets = np.array([1 if file.startswith('HAM') else 0 for file in os.listdir(mail_dir)])\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "targets = extract_targets(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the file explorer sorts the files alphabetically in each directory, all the ham emails appear before the spam. This would be a problem if I was using an index feature of some sort, because the classifier would associate ham emails with lower index numbers and spam with higher ones. Since I'm not, the targets appearing like this is fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection, Training, and Evaluation\n",
    "\n",
    "With the emails all processed and sitting pretty in a feature matrix, it's time to see if we can build a great classifier using machine learning algorithms. I'm going to experiment using a Support Vector Machine and a Decision Tree, two powerful algorithms that are well suited to high dimensionality feature space. The Decision Tree will also let me see the relative importance of the word features in making a classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf = SVC()\n",
    "svm_clf.fit(X, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99750000000000005"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.score(X, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right off the bat, we can see that the SVC fit the training set nearly perfectly. It seems as though classifying these emails may be pretty straightforward... To check for overfit, lets do some cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc_scores = cross_val_score(svm_clf, X, targets, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print('Scores:', scores)\n",
    "    print('Mean:', scores.mean())\n",
    "    print('Standard Deviation:', scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [ 0.9689441   0.98757764  0.98125     0.9875      1.          1.          0.99375\n",
      "  0.9875      0.99371069  1.        ]\n",
      "Mean: 0.990023243095\n",
      "Standard Deviation: 0.00930649027043\n"
     ]
    }
   ],
   "source": [
    "display_scores(svc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVC still holds up really well, with the lowest accuracy on one of the folds being ~97%. With a standard deviation of less than 1%, we can see that the SVC boundary indeed is also a perfect fit on this training data. Let's see how the Decision Tree fares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_clf.fit(X, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.score(X, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perfect fit. Not surprising given the SVC; lets check for overfit with cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree_scores = cross_val_score(tree_clf, X, targets, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [ 0.98757764  0.88819876  1.          1.          1.          1.          1.\n",
      "  1.          1.          1.        ]\n",
      "Mean: 0.987577639752\n",
      "Standard Deviation: 0.0333326903478\n"
     ]
    }
   ],
   "source": [
    "display_scores(tree_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting... 9 out of the 10 folds basically scored perfectly, but one fold had a pretty significant 10% dip in accuracy, dropping to ~89%. Perhaps the absence of a particular fold of emails, with particular feature vectors, made all the difference.\n",
    "\n",
    "## Exploratory Feature Analysis\n",
    "\n",
    "To see which word features most significantly contributed to the Decision Tree's classifications, we can check the feature importances field in the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_importances = tree_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pair the importances with the word dictionary (they share the same index) and reverse the list to see which ones had the greatest contributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.68473698548256134, ('IMAP', 967)),\n",
       " (0.30778663520333421, ('Jul', 2062)),\n",
       " (0.002498212216882349, ('unknown', 275)),\n",
       " (0.0024921190163532041, ('From', 1543)),\n",
       " (0.0024860480808689676, ('MOST', 15)),\n",
       " (0.0, ('zzzzteana', 14)),\n",
       " (0.0, ('young', 20)),\n",
       " (0.0, ('yet', 61)),\n",
       " (0.0, ('yesterday', 15)),\n",
       " (0.0, ('yes', 15)),\n",
       " (0.0, ('years', 294)),\n",
       " (0.0, ('year', 144)),\n",
       " (0.0, ('wrote', 33)),\n",
       " (0.0, ('wrong', 38)),\n",
       " (0.0, ('written', 38)),\n",
       " (0.0, ('writing', 50)),\n",
       " (0.0, ('write', 88)),\n",
       " (0.0, ('would', 791)),\n",
       " (0.0, ('worth', 60)),\n",
       " (0.0, ('worst', 14)),\n",
       " (0.0, ('worry', 27)),\n",
       " (0.0, ('worldwide', 21)),\n",
       " (0.0, ('world', 165)),\n",
       " (0.0, ('works', 95)),\n",
       " (0.0, ('working', 176)),\n",
       " (0.0, ('workers', 17)),\n",
       " (0.0, ('worked', 50)),\n",
       " (0.0, ('work', 337)),\n",
       " (0.0, ('words', 62)),\n",
       " (0.0, ('word', 113))]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(feature_importances, word_dictionary), reverse=True)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... Only 5 out of the 3000 features created actually played into the Decision Tree's classifications. And only two of them had any substantial impact: 'IMAP' and 'Jul', with both 68% and 30% importances respectively. The presence of these two words seemed to give the model almost all the information it needed to make a correct prediction.\n",
    "\n",
    "Let's see the frequency in which these words appeared in emails overall, and how they're used in them. The function below parses a directory of emails and returns a list of those which contain the keyword we're looking for. It also prints the number and percentage of the total files in which it occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_keyword_files(keyword, mail_dir):\n",
    "    size = len(os.listdir(mail_dir))\n",
    "    files_with_keyword = []\n",
    "    files = [os.path.join(mail_dir, file) for file in os.listdir(mail_dir)]\n",
    "    for file in files:\n",
    "        with open(file, 'r') as f:\n",
    "            content = f.readlines()\n",
    "            for line in content:\n",
    "                words = line.split(' ')\n",
    "                if keyword in words:\n",
    "                    if file in files_with_keyword: # prevent duplicate entries in files_with_keyword\n",
    "                        pass\n",
    "                    else:\n",
    "                        #print('\"{}\" found in file: {}'.format(keyword, file))\n",
    "                        files_with_keyword.append(file)\n",
    "        f.close()\n",
    "    \n",
    "    print((\"'{}' found in {} of {} files ({})\").format(keyword, len(files_with_keyword), size, len(files_with_keyword)/size))\n",
    "\n",
    "    return files_with_keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'IMAP' found in 967 of 1600 files (0.604375)\n"
     ]
    }
   ],
   "source": [
    "imap_files = find_keyword_files(\"IMAP\", train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'IMAP' is present in over half of the training set - but is it in mostly ham or spam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham: 818 (0.8459152016546019)\n",
      "Spam: 149 (0.15408479834539815)\n"
     ]
    }
   ],
   "source": [
    "imap_ham = 0\n",
    "imap_spam = 0\n",
    "for file in imap_files:\n",
    "    if 'HAM' in file:\n",
    "        imap_ham += 1\n",
    "    else:\n",
    "        imap_spam += 1\n",
    "print(('Ham: {} ({})').format(imap_ham, imap_ham/len(imap_files)))\n",
    "print(('Spam: {} ({})').format(imap_spam, imap_spam/len(imap_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'IMAP' appears mostly in ham emails. Let's read one and see if we can find where its used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From exmh-workers-admin@redhat.com  Thu Aug 22 12:36:23 2002\n",
      "\n",
      "Return-Path: <exmh-workers-admin@example.com>\n",
      "\n",
      "Delivered-To: zzzz@localhost.netnoteinc.com\n",
      "\n",
      "Received: from localhost (localhost [127.0.0.1])\n",
      "\n",
      "\tby phobos.labs.netnoteinc.com (Postfix) with ESMTP id D03E543C36\n",
      "\n",
      "\tfor <zzzz@localhost>; Thu, 22 Aug 2002 07:36:16 -0400 (EDT)\n",
      "\n",
      "Received: from phobos [127.0.0.1]\n",
      "\n",
      "\tby localhost with IMAP (fetchmail-5.9.0)\n",
      "\n",
      "\tfor zzzz@localhost (single-drop); Thu, 22 Aug 2002 12:36:16 +0100 (IST)\n",
      "\n",
      "Received: from listman.example.com (listman.example.com [66.187.233.211]) by\n",
      "\n",
      "    dogma.slashnull.org (8.11.6/8.11.6) with ESMTP id g7MBYrZ04811 for\n",
      "\n",
      "    <zzzz-exmh@example.com>; Thu, 22 Aug 2002 12:34:53 +0100\n",
      "\n",
      "Received: from listman.example.com (localhost.localdomain [127.0.0.1]) by\n",
      "\n",
      "    listman.redhat.com (Postfix) with ESMTP id 8386540858; Thu, 22 Aug 2002\n",
      "\n",
      "    07:35:02 -0400 (EDT)\n",
      "\n",
      "Delivered-To: exmh-workers@listman.example.com\n",
      "\n",
      "Received: from int-mx1.corp.example.com (int-mx1.corp.example.com\n",
      "\n",
      "    [172.16.52.254]) by listman.redhat.com (Postfix) with ESMTP id 10CF8406D7\n",
      "\n",
      "    for <exmh-workers@listman.redhat.com>; Thu, 22 Aug 2002 07:34:10 -0400\n",
      "\n",
      "    (EDT)\n",
      "\n",
      "Received: (from mail@localhost) by int-mx1.corp.example.com (8.11.6/8.11.6)\n",
      "\n",
      "    id g7MBY7g11259 for exmh-workers@listman.redhat.com; Thu, 22 Aug 2002\n",
      "\n",
      "    07:34:07 -0400\n",
      "\n",
      "Received: from mx1.example.com (mx1.example.com [172.16.48.31]) by\n",
      "\n",
      "    int-mx1.corp.redhat.com (8.11.6/8.11.6) with SMTP id g7MBY7Y11255 for\n",
      "\n",
      "    <exmh-workers@redhat.com>; Thu, 22 Aug 2002 07:34:07 -0400\n",
      "\n",
      "Received: from ratree.psu.ac.th ([202.28.97.6]) by mx1.example.com\n",
      "\n",
      "    (8.11.6/8.11.6) with SMTP id g7MBIhl25223 for <exmh-workers@redhat.com>;\n",
      "\n",
      "    Thu, 22 Aug 2002 07:18:55 -0400\n",
      "\n",
      "Received: from delta.cs.mu.OZ.AU (delta.coe.psu.ac.th [172.30.0.98]) by\n",
      "\n",
      "    ratree.psu.ac.th (8.11.6/8.11.6) with ESMTP id g7MBWel29762;\n",
      "\n",
      "    Thu, 22 Aug 2002 18:32:40 +0700 (ICT)\n",
      "\n",
      "Received: from munnari.OZ.AU (localhost [127.0.0.1]) by delta.cs.mu.OZ.AU\n",
      "\n",
      "    (8.11.6/8.11.6) with ESMTP id g7MBQPW13260; Thu, 22 Aug 2002 18:26:25\n",
      "\n",
      "    +0700 (ICT)\n",
      "\n",
      "From: Robert Elz <kre@munnari.OZ.AU>\n",
      "\n",
      "To: Chris Garrigues <cwg-dated-1030377287.06fa6d@DeepEddy.Com>\n",
      "\n",
      "Cc: exmh-workers@example.com\n",
      "\n",
      "Subject: Re: New Sequences Window\n",
      "\n",
      "In-Reply-To: <1029945287.4797.TMDA@deepeddy.vircio.com>\n",
      "\n",
      "References: <1029945287.4797.TMDA@deepeddy.vircio.com>\n",
      "\n",
      "    <1029882468.3116.TMDA@deepeddy.vircio.com> <9627.1029933001@munnari.OZ.AU>\n",
      "\n",
      "    <1029943066.26919.TMDA@deepeddy.vircio.com>\n",
      "\n",
      "    <1029944441.398.TMDA@deepeddy.vircio.com>\n",
      "\n",
      "MIME-Version: 1.0\n",
      "\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "\n",
      "Message-Id: <13258.1030015585@munnari.OZ.AU>\n",
      "\n",
      "X-Loop: exmh-workers@example.com\n",
      "\n",
      "Sender: exmh-workers-admin@example.com\n",
      "\n",
      "Errors-To: exmh-workers-admin@example.com\n",
      "\n",
      "X-Beenthere: exmh-workers@example.com\n",
      "\n",
      "X-Mailman-Version: 2.0.1\n",
      "\n",
      "Precedence: bulk\n",
      "\n",
      "List-Help: <mailto:exmh-workers-request@example.com?subject=help>\n",
      "\n",
      "List-Post: <mailto:exmh-workers@example.com>\n",
      "\n",
      "List-Subscribe: <https://listman.example.com/mailman/listinfo/exmh-workers>,\n",
      "\n",
      "    <mailto:exmh-workers-request@redhat.com?subject=subscribe>\n",
      "\n",
      "List-Id: Discussion list for EXMH developers <exmh-workers.example.com>\n",
      "\n",
      "List-Unsubscribe: <https://listman.example.com/mailman/listinfo/exmh-workers>,\n",
      "\n",
      "    <mailto:exmh-workers-request@redhat.com?subject=unsubscribe>\n",
      "\n",
      "List-Archive: <https://listman.example.com/mailman/private/exmh-workers/>\n",
      "\n",
      "Date: Thu, 22 Aug 2002 18:26:25 +0700\n",
      "\n",
      "\n",
      "\n",
      "    Date:        Wed, 21 Aug 2002 10:54:46 -0500\n",
      "\n",
      "    From:        Chris Garrigues <cwg-dated-1030377287.06fa6d@DeepEddy.Com>\n",
      "\n",
      "    Message-ID:  <1029945287.4797.TMDA@deepeddy.vircio.com>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  | I can't reproduce this error.\n",
      "\n",
      "\n",
      "\n",
      "For me it is very repeatable... (like every time, without fail).\n",
      "\n",
      "\n",
      "\n",
      "This is the debug log of the pick happening ...\n",
      "\n",
      "\n",
      "\n",
      "18:19:03 Pick_It {exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace} {4852-4852 -sequence mercury}\n",
      "\n",
      "18:19:03 exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace 4852-4852 -sequence mercury\n",
      "\n",
      "18:19:04 Ftoc_PickMsgs {{1 hit}}\n",
      "\n",
      "18:19:04 Marking 1 hits\n",
      "\n",
      "18:19:04 tkerror: syntax error in expression \"int ...\n",
      "\n",
      "\n",
      "\n",
      "Note, if I run the pick command by hand ...\n",
      "\n",
      "\n",
      "\n",
      "delta$ pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace  4852-4852 -sequence mercury\n",
      "\n",
      "1 hit\n",
      "\n",
      "\n",
      "\n",
      "That's where the \"1 hit\" comes from (obviously).  The version of nmh I'm\n",
      "\n",
      "using is ...\n",
      "\n",
      "\n",
      "\n",
      "delta$ pick -version\n",
      "\n",
      "pick -- nmh-1.0.4 [compiled on fuchsia.cs.mu.OZ.AU at Sun Mar 17 14:55:56 ICT 2002]\n",
      "\n",
      "\n",
      "\n",
      "And the relevant part of my .mh_profile ...\n",
      "\n",
      "\n",
      "\n",
      "delta$ mhparam pick\n",
      "\n",
      "-seq sel -list\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Since the pick command works, the sequence (actually, both of them, the\n",
      "\n",
      "one that's explicit on the command line, from the search popup, and the\n",
      "\n",
      "one that comes from .mh_profile) do get created.\n",
      "\n",
      "\n",
      "\n",
      "kre\n",
      "\n",
      "\n",
      "\n",
      "ps: this is still using the version of the code form a day ago, I haven't\n",
      "\n",
      "been able to reach the cvs repository today (local routing issue I think).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________\n",
      "\n",
      "Exmh-workers mailing list\n",
      "\n",
      "Exmh-workers@redhat.com\n",
      "\n",
      "https://listman.redhat.com/mailman/listinfo/exmh-workers\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_email(imap_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this email, it appears right by the top. Its in the 'Received:' field of the email - \"by localhost with IMAP (fetchmail-5.9.0)\". Doing a little bit of research online, I found the following info on IMAP:\n",
    "\n",
    "\"IMAP (Internet Message Access Protocol) is a standard email protocol that stores email messages on a mail server, but allows the end user to view and manipulate the messages as though they were stored locally on the end user's computing device(s). This allows users to organize messages into folders, have multiple client applications know which messages have been read, flag messages for urgency or follow-up and save draft messages on the server.\" \n",
    "\n",
    "\\-taken from http://searchexchange.techtarget.com/definition/IMAP\n",
    "\n",
    "I suppose that emails sent through this IMAP protocol are more likely to be ham then spam! I could do the same analysis for the other significant word, 'Jul', but after looking into it I realized that it only represents the date which the email was sent. Not so much indicative of an email being spam or ham, just when most of the emails in this dataset were gathered.\n",
    "\n",
    "## Evaluate the models on the Test Set\n",
    "\n",
    "Moving onto the final step, testing our near perfect classifiers on the Test Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 files processed\n",
      "200 files processed\n",
      "300 files processed\n",
      "400 files processed\n"
     ]
    }
   ],
   "source": [
    "test_X = extract_features(test_path, word_dictionary)\n",
    "test_y = extract_targets(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_y_svm = svm_clf.predict(test_X)\n",
    "pred_y_tree = tree_clf.predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the predictions for each classifier look like. Since all ham files come before spam alphabetically, a perfect classifier should output two discrete blocks of 1s and 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good! Looks like the SVM incorrectly classified two spam emails as ham. Let's see its overall accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.995"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_y == pred_y_svm).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Near perfect... Moving on to the Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the Decision Tree made two incorrect classifications on spam emails as well (perhaps due to them containing 'IMAP'). Its accuracy should be the same as the SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.995"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_y == pred_y_tree).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, it seems as though this dataset can quite easily be handled by out-of-the-box models for ham/spam classification. Since the accuracies were so high for each model, I didn't do any tweaking of hyperparameters to try and improve them. I also ignored checking for precision or recall, since at ~99% accuracy they're both pretty obvious. While an interesting exercise, it says little about the viability of these models in an real online environment. The algorithms I used to extract the features weren't even good enough to judge the emails solely on their contents - the two most important features in the Decision Tree were contained in the email headers. Future, improved versions of this exercise would involve a much larger dataset of more diverse emails, and an improved algorithm for extracting the word features.\n",
    "\n",
    "Thanks for reading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
